{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle, re, csv, string, getopt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, pprint, csv\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.special import gammaln, psi\n",
    "#set path!\n",
    "sys.path.append('/Users/oleksiyostapenko/Documents/HU_Berlin/DataModeling/Practical/ldasvi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doc_1 = \"Brocolli is good to eat. My Brocolli brother likes Brocolli to eat good brocolli, but not my mother.\"\n",
    "#doc_2 = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "#doc_3 = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "#doc_4 = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "#doc_5 = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "#docset = [doc_1, doc_2, doc_3, doc_4, doc_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dirichlet_expectation_log(alpha):\n",
    "    if (len(alpha.shape) == 1):\n",
    "        return(psi(alpha) - psi(np.sum(alpha)))\n",
    "    return(psi(alpha) - psi(np.sum(alpha, 1))[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dir_expectation(alpha):\n",
    "    return(  alpha / np.sum(alpha, 1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_doc_list(docs, vocab):\n",
    "    D = len(docs)\n",
    "    wordids = list()\n",
    "    wordcts = list()\n",
    "    for d in range(0, D):\n",
    "        words = string.split(docs[d])\n",
    "        ddict = dict()\n",
    "        for word in words:\n",
    "            if (word in vocab):\n",
    "                wordtoken = vocab[word]\n",
    "                if (not wordtoken in ddict):\n",
    "                    ddict[wordtoken] = 0\n",
    "                ddict[wordtoken] += 1\n",
    "        wordids.append(ddict.keys())\n",
    "        wordcts.append(ddict.values())\n",
    "\n",
    "    return((wordids, wordcts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#f = open('en_voc.txt', 'w')\n",
    "#for word in voc:\n",
    "#    f.write(word+'\\n')\n",
    "#f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One iteration of the outer loop for batch stochastic variational inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_iteration_batch( rhot, word_ids, word_counts, gamma, _Elogbeta,  _lambda, K, D, alpha, eta, batch_length):\n",
    "    \"\"\"\n",
    "    One iteration of the outer loop for batch stochastic variational inference.\n",
    "    returns optimal gama and lambda for the doc batch\n",
    "    \"\"\"\n",
    "    Elogtheta = dirichlet_expectation_log(gamma)\n",
    "    expElogbeta=np.exp(_Elogbeta)\n",
    "    \n",
    "    for doc in range(0, batch_length):\n",
    "        \n",
    "        ids = word_ids[doc]\n",
    "        cts = word_counts[doc]\n",
    "            \n",
    "        #w_dn=np.zeros(_lambda.shape[1])\n",
    "        w_dn_new=np.zeros(_lambda.shape)\n",
    "        \n",
    "        Elogtheta_doc=Elogtheta[doc,:]\n",
    "        expElogtheta_doc = np.exp(Elogtheta_doc)\n",
    "        gamma_doc=gamma[doc,:]\n",
    "        \n",
    "        expElogbeta_doc=expElogbeta[:,ids]\n",
    "    \n",
    "        lambdas_temp=np.array(np.zeros(_lambda.shape))\n",
    "        phinorm = np.dot(expElogtheta_doc, expElogbeta_doc)\n",
    "        #print(phinorm.shape)\n",
    "        #print(phinorm)\n",
    "        \n",
    "        for it in range(0, 200):\n",
    "                #print(it)\n",
    "                lastgamma = gamma_doc\n",
    "                \n",
    "                # calculate phi, parameter for the topic distribution for each word\n",
    "                \n",
    "                #phis_temp=np.array(np.empty((0,K)))\n",
    "                #phi_kdn=expElogthetad*expElogbetad\n",
    "                \n",
    "                phis_temp= expElogtheta_doc*((cts / phinorm) * expElogbeta_doc).T  #  same as:\n",
    "                #print(\"sume\")\n",
    "                #print(np.sum(phis_temp, axis=0))\n",
    "                \n",
    "                #for n in range(0, len(ids)): #for each word\n",
    "                        #phi_kdn=expElogthetad*expElogbetad[:,n]     # <- because for real numbers exp(x+y)=(expx)(expy)                   \n",
    "                        \n",
    "                        #print(type(expElogbetad))\n",
    "                        #print(type(expElogthetad))\n",
    "                        \n",
    "                        #phi_kdn=np.exp(Elogthetad + Elogbetad[:,n])\n",
    "                        #phis_temp=np.append(phis_temp, np.array([phi_kdn]), axis=0)\n",
    "                        \n",
    "                \n",
    "                gamma_doc = alpha + np.sum(phis_temp, axis=0)\n",
    "                #gamma_doc=alpha+phis_temp\n",
    "                #print(gamma_doc.shape)\n",
    "                #print(gamma_doc)\n",
    "                \n",
    "                #redefine theta\n",
    "                #print(gamma_doc.shape)\n",
    "                Elogtheta_doc = dirichlet_expectation_log(gamma_doc)\n",
    "                #Elogtheta_doc[Elogtheta_doc==np.inf]=0\n",
    "                expElogtheta_doc = np.exp(Elogtheta_doc)\n",
    "                phinorm = np.dot(expElogtheta_doc, expElogbeta_doc) + 1e-100\n",
    "                \n",
    "                \n",
    "                change = np.mean(abs(gamma_doc - lastgamma))\n",
    "                #print(change)\n",
    "                if (change< threshold):\n",
    "                    break\n",
    "        gamma[doc,:]=gamma_doc\n",
    "        #print(phis_temp.shape)\n",
    "        #print(gamma_doc.shape)\n",
    "        \n",
    "        #calculate intermediate topics\n",
    "        w_dn_new[:,ids]=phis_temp.T\n",
    "        #print(phis_temp.shape)\n",
    "        lambda_hat_temp=eta+D*w_dn_new # same as the following for loop:        \n",
    "        #print(lambda_hat_temp.shape)\n",
    "        #lambda_temp=np.array(np.empty((0,len(w_dn))))\n",
    "        #for k in range(0,K):\n",
    "        #    w_dn[ids]=phis_temp[:,k]\n",
    "        #    lambda_hat=eta+D*w_dn\n",
    "        #    lambda_temp=np.append(lambda_temp, np.array([lambda_hat]), axis=0)\n",
    "       \n",
    "        # summ all lambdas for the batch in a matrix\n",
    "        lambdas_temp=lambdas_temp+lambda_hat_temp\n",
    "        #print(lambdas_temp.shape)\n",
    "    _lambda = (1- rhot)*_lambda + rhot/batch_length * lambdas_temp  #equation page 1321, 2.5. Extentions\n",
    "    #print(_lambda.shape)\n",
    "    return(gamma, _lambda)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from stop_words import get_stop_words\n",
    "#en_stop = get_stop_words('en')\n",
    "    # create English stop words list\n",
    "#en_stop = get_stop_words('en')\n",
    "#voc=[]\n",
    "#for word in range(0, len(vocab)):\n",
    "#    if not vocab[word] in en_stop:\n",
    "#        vocab[word] = vocab[word].lower()\n",
    "#        if not vocab[word] in en_stop:\n",
    "#            vocab[word] = re.sub(r'[^a-z]', '', vocab[word])\n",
    "#            if not vocab[word] in en_stop and len(vocab[word])>2:\n",
    "#                voc.append(vocab[word])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab(file_dest):\n",
    "    vocab = file('en_voc_most_comon_no_stop.txt').readlines()\n",
    "    _vocab = dict()\n",
    "    for word in vocab:\n",
    "        word = word.lower()\n",
    "        word = re.sub(r'[^a-z]', '', word)\n",
    "        _vocab[word] = len(_vocab)\n",
    "    return((_vocab, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showtopics(vocab, _lambda, number_of_words):\n",
    "    for k in range(0, len(_lambda)):\n",
    "        lambdak=list(_lambda[k, :])\n",
    "        lambdak=lambdak/sum(lambdak)\n",
    "        temp=zip(lambdak, range(0, len(lambdak)))\n",
    "        temp=sorted(temp, key = lambda x: x[0], reverse=True)\n",
    "        print 'topic %d:\\n' % (k)\n",
    "        for i in range(0, number_of_words):\n",
    "            #print '%s  %.5f' % (vocab[temp[i][1]], temp[i][0])\n",
    "            print '%s ' % vocab[temp[i][1]]\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import csv\n",
    "\n",
    "#myfile = open(\"hillary_test\", 'wb')\n",
    "#wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#wr.writerow(docset_test)\n",
    "#myfile = open(\"hillary_train\", 'wb')\n",
    "#wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#wr.writerow(docset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictive_distribution_for_test(_lambdas_learned, word_ids_seen_unseen, counts_seen_unseen):\n",
    "    \"\"\"\n",
    "    returns the log predictive distribution for a document\n",
    "    \"\"\"\n",
    "    log_pred_probs=[]\n",
    "    for doc_test in range(0, len(word_ids_seen_unseen)):\n",
    "        if (doc_test%100 ==0):\n",
    "            print(\"doc_test %s\" %doc_test)\n",
    "        word_ids_seen=word_ids_seen_unseen[doc_test][0]\n",
    "        word_ids_unseen=word_ids_seen_unseen[doc_test][1]\n",
    "        word_counts_seen=counts_seen_unseen[doc_test][0]\n",
    "        word_counts_unseen=counts_seen_unseen[doc_test][1]\n",
    "        \n",
    "        gamma_test = np.random.gamma(100., 1./100., (1, K))  # define variational distribution for topic proportions per document\n",
    "        #print(gamma_test[0,:].shape)\n",
    "        Elogbeta_learned = dirichlet_expectation_log(_lambdas_learned)  # E(log(Beta_k|lambda)) \n",
    "        rhot_t = pow(1 + tau_0, -ka)\n",
    "        (gamma, _lambda) = one_iteration_batch(rhot_t, [word_ids_seen], [word_counts_seen], gamma_test, Elogbeta_learned,_lambdas_learned, K, D, alpha, eta, 1)\n",
    "        Ebeta = dirichlet_expectation_log(_lambda)\n",
    "        Etheta = dirichlet_expectation_log(gamma[0])\n",
    "        p_Wnew_given_Wobs_D= np.log(np.dot(Etheta,Ebeta))\n",
    "        #print(p_Wnew_given_Wobs_D.shape)\n",
    "        log_pred_prob=[p_Wnew_given_Wobs_D[i] for i in word_ids_unseen]\n",
    "        #print(log_pred_prob)\n",
    "        log_pred_probs.append(log_pred_prob)\n",
    "    return(log_pred_probs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-19a2b2ebdd85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdocset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindezies_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#prepare test docset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mword_ids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_counts_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_doc_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#devide test data in W_obs and W_new <- paper chapter 4, page 1336\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ids_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_dic' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('hillary-clinton-emails/Emails.csv', sep=',',header=None)\n",
    "docset=df[21].tolist()\n",
    "for word in range(0,len(docset)):\n",
    "    docset[word] = docset[word].lower()\n",
    "    docset[word] = re.sub(r'[^a-z]', ' ', docset[word])\n",
    "#devide in train and test docsets\n",
    "len_test=int((0.1*len(docset)))\n",
    "indezies_test=np.random.randint(len(docset), size=len_test).tolist()\n",
    "docset_test=[docset[i] for i in indezies_test]\n",
    "docset_train = [docset[i] for i in range(0, len(docset)) if i not in indezies_test]\n",
    "#prepare test docset\n",
    "(word_ids_test, word_counts_test) = parse_doc_list(docset_test, vocab_dic)\n",
    "#devide test data in W_obs and W_new <- paper chapter 4, page 1336\n",
    "for doc_i in range(0,len(word_ids_test)):\n",
    "    last_seen_in_doc=int(0.7*len(word_ids_test[doc_i]))\n",
    "    W_obs=[word_ids_test[doc_i][i] for i in range(0, last_seen_in_doc)]\n",
    "    Counts_obs=[word_counts_test[doc_i][i] for i in range(0, last_seen_in_doc)]\n",
    "    \n",
    "    W_new=[word_ids_test[doc_i][i] for i in range(last_seen_in_doc, len(word_ids_test[doc_i])) if word_ids_test[doc_i][i] not in W_obs]\n",
    "    Counts_new=[word_counts_test[doc_i][i] for i in range(last_seen_in_doc, len(word_ids_test[doc_i])) if word_ids_test[doc_i][i] not in W_obs]\n",
    "    \n",
    "    word_ids_test[doc_i]=[W_obs, W_new]\n",
    "    word_counts_test[doc_i]=[Counts_obs, Counts_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = len(docset) # number of documents \n",
    "#D = 1000000\n",
    "K = 50 # number of topics\n",
    "np.random.seed(555)\n",
    "threshold=0.0001\n",
    "    \n",
    "(vocab_dic, vocab)=get_vocab(\"vocabulary.txt\")\n",
    "    \n",
    "W = len(vocab_dic)\n",
    "\n",
    "alpha=1./K\n",
    "eta=1./K\n",
    "tau_0=1000.\n",
    "ka =0.7\n",
    "\n",
    "mean_log_probs=[]\n",
    "iterations = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run(batch_size):\n",
    "    \n",
    "    # initialize lambda randomlz\n",
    "    _lambda = np.random.gamma(1000., 1./100., (K, W))  # distribution for topics over the vocabualry\n",
    "    Elogbeta = dirichlet_expectation_log(_lambda)  # E(log(Beta_k|lambda)) \n",
    "    \n",
    "    mean_log_probs_batch=[]\n",
    "\n",
    "    for iteration in range(0, iterations+1):\n",
    "        # print(iteration)\n",
    "        # define step size\n",
    "        rhot_t = pow(iteration + tau_0, -ka) # Equation 26 in the paper\n",
    "\n",
    "        #doc = docset[randint(0, len(docset) - 1)]\n",
    "        \n",
    "        docs = [docset_train[i] for i in np.random.randint(len(docset_train), size=batch_size).tolist()]        \n",
    "    \n",
    "        for doci in range(0,len(docs)):\n",
    "            docs[doci] = docs[doci].lower()\n",
    "            docs[doci]  = re.sub(r'[^a-z]', ' ', docs[doci])\n",
    "        \n",
    "        \n",
    "        (word_ids, word_counts) = parse_doc_list(docs, vocab_dic)\n",
    "        \n",
    "        gamma = np.random.gamma(1000., 1./100., (batch_size, K))  # define variational distribution for topic proportions per document\n",
    "        (gamma, _lambda) = one_iteration_batch(rhot_t, word_ids, word_counts, gamma, Elogbeta,_lambda, K, D, alpha, eta, batch_size)\n",
    "        \n",
    "        #(gamma, _lambda) = one_iteration(rhot, updatect, kappa, word_ids, word_counts, gamma, Elogbeta, _lambda, K, D, alpha, eta)\n",
    "\n",
    "        Elogbeta = dirichlet_expectation_log(_lambda)  # E(log(Beta_k|lambda))\n",
    "\n",
    "        if (iteration % 100 == 0) and not (iteration==300):\n",
    "            print(\"write\")\n",
    "            print(\"iteration %d\" % iteration)\n",
    "            np.savetxt('lambda-2-%d-%d.dat' % (iteration,batch_size), _lambda)\n",
    "            np.savetxt('gamma-%d-%d.dat' % (iteration, batch_size), gamma)\n",
    "            _lambdas_learned=_lambda\n",
    "            probs=np.asarray(predictive_distribution_for_test(_lambdas_learned, word_ids_test, word_counts_test))\n",
    "            mean_log_prob=np.mean(np.asarray([np.mean(probs[i]) for i in range(0,len(probs))]))\n",
    "            mean_log_probs_batch.append(mean_log_prob)\n",
    "            \n",
    "            if (iteration% 1000 == 0):\n",
    "                np.savetxt('log_probs-2-%d-%d.dat' % (iteration, batch_size), np.asarray(mean_log_probs_batch))\n",
    "                if (iteration == iterations):\n",
    "                    showtopics(vocab,_lambda)\n",
    "    return(mean_log_probs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(batch_size):\n",
    "    for i in range(0,len(batch_size)): #calculate log pred distr for each batch size\n",
    "        print(\"batch %d\" % i)\n",
    "        mean_log_probs_batch=run(batch_size[i])\n",
    "        mean_log_probs.append(mean_log_probs_batch)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # if batch_size==1, analyse 1 doc per iteration\n",
    "    \n",
    "    df=pd.read_csv('hillary-clinton-emails/Emails.csv', sep=',',header=None)\n",
    "    docset=df[21].tolist()\n",
    "    for word in range(0,len(docset)):\n",
    "        docset[word] = docset[word].lower()\n",
    "        docset[word] = re.sub(r'[^a-z]', ' ', docset[word])\n",
    "    \n",
    "    main(batch_size=[1,50,100,1000])\n",
    "    #main(batch_size=[100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from her: printing the learned topics and ploting the log predictive distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = str.split(file(\"vocabulary.txt\").read())\n",
    "t_lambda = np.loadtxt(\"lambda-2-5000-100.dat\")\n",
    "showtopics(vocab,t_lambda, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mean_log_prob_200=file('/Users/oleksiyostapenko/Documents/HU_Berlin/DataModeling/Practical/hillary_D10000_100batch/log_probs-2-5000.dat').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=[i*100 for i in range(0,20)]\n",
    "batch_1=plt.plot(a,mean_log_probs[0], label=\"batch size =1\")\n",
    "batch_50=plt.plot(a,mean_log_probs[1], label=\"batch size = 50\")\n",
    "batch_100=plt.plot(a,mean_log_probs[2], label=\"batch size = 100\")\n",
    "batch_1000=plt.plot(a,mean_log_probs[3], label=\"batch size = 1000\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "#plt.plot([1,2,3],[1,2,3])\n",
    "plt.ylabel('log predictive likelihood')\n",
    "plt.xlabel('number of iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
